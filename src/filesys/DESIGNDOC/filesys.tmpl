       	       	     +-------------------------+
                     |          CS 124         |
                     | PROJECT 6: FILE SYSTEMS |
                     |     DESIGN DOCUMENT     |
                     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Vaibhav Anand <vanand@caltech.edu>
Nikhil  Gupta <nkgupta@caltech.edu>
Michael Hashe <mhashe@caltech.edu>

>> Specify how many late tokens you are using on this assignment:  

0

>> What is the Git repository and commit hash for your submission?

    Repository URL:  https://github.com/MichaelHashe/CS124-OS
    commit ...

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course instructors.

Stackover flow, wikipedia

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Lock on accessing cache table. */
struct lock cache_table_lock;

/* Cache of data */
static struct cache_entry sector_cache[CACHE_SIZE];

/* Read-ahead queue, clock-like index. */
static int read_ahead_buffer[CACHE_SIZE];
volatile int read_ahead_head;   /* Index for reading from queue. */
volatile int read_ahead_tail;   /* Index for writing to queue. */

/* LRU cache. */
static struct list cache_lru;


enum lock_mode {
    UNLOCK,                         /* No one occupies lock. */
    READ_LOCK,                      /* Readers occupy lock. */
    WRITE_LOCK                      /* Writer occupies lock. */
};

struct lru_entry {
    block_sector_t sector;          /* Sector number. */

    struct list_elem elem;          /* Required for list. */
};

struct cache_entry {
    volatile int sector;            /* Sector number loaded into cache. */
    bool access;                    /* Has sector been accessed. */
    bool dirty;                     /* Has sector been written to. */
    char data[BLOCK_SECTOR_SIZE];   /* Actual sector data. */

    /* Implement read/write lock. */
    struct lock cache_entry_lock;

    struct condition readers;       /* Condvar for readers. */
    struct condition writers;       /* Condvar for writers. */

    /* Count of waiting processes. */
    uint8_t reader_active;          /* Threads currently reading. */
    uint8_t reader_waiting;         /* Threads waiting to read. */
    /* Writer_active should only ever be one or zero. */
    uint8_t writer_waiting;         /* Threads waiting to write. */

    enum lock_mode mode;            /* Who currently holds lock. */
};

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We use an LRU replacement policy. Instead of checking access bits every
timer tick, we check before each eviction (this is largely to avoid 
concurrency issues). At each eviction, the block at the tail of the 
LRU queue is evicted, and a new block is appended to the front.

>> C3: Describe your implementation of write-behind.

Write behind is a kernel thread that gets started on system init. The thread
goes through the whole cache table and writes each sector (if it is dirty) to
disk, clearing its dirty bit. It does this while holding the cache table lock.
When it finds a dirty sector, it obtains that cache slot's lock (possibly
blocking on it), writes it to disk, and then releases the lock. After it has
gone through the whole table, it releases the cache table lock and sleeps.

>> C4: Describe your implementation of read-ahead.

The read ahead implementation is very similar to the write behind
implementation. It reads through its queue while holding the cache table lock.
It checks to see if the sector has already been loaded into the cache. If it
has, then it does nothing for that slot. If it hasn't, it tries loading it
into the cache, evicting something if necessary. After it goes through the
whole queue, it releases the cache table lock and sleeps.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

When a process is using the cache block, it holds that cache block's lock. If
there are multiple readers using it, the last reader that finishes (as kept
track of by a count) releases the R/W lock, and either gives it to a writer or
marks the block as unlocked. As such, when a process is actively reading or
writing a slot, it necessarily holds the lock on that slot.

On eviction, once a victim (a cache slot) is chosen, we acquire that slot's
lock. This means that the eviction cannot procede until it acquires the lock
which means that no one else can be actively using it. As such, an eviction
cannot occur when another process is reading and writing. 

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

This follows a very similar logic. On eviction, once a victim is chosen, we
require the cache slot's lock. Any r/w operations on a slot require this same
lock so they cannot procede until they acquire the lock. In order to prevnt the
sector that is loaded in a certain slot from being pulled out from under the r/w
operation, once we acquire a certain slot's lock, we check to make sure that it
is still the sector we want. If it isn't, we then release the lock and while
loop and try again.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

Repeated accesses to the same file(s) is likely to benefit from caching. Read-
ahead is likely to benefit sequential scans, i.e. for data processing. Write-
behind is likely to benefit sequentual writes, in making eviction operataions
less costly.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the feedback survey on the course
website.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?

